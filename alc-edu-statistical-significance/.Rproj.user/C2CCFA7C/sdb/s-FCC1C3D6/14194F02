{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Math 154, Take Home Exam 2\" \nauthor: \"Julian DeGroot-Lutzner\" \ndate: \"due 11/22/2017\"\noutput: pdf_document \n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message=FALSE,\nwarning=FALSE, cache=TRUE, fig.width=7, fig.height=4, fig.align = \"center\") \noptions(digits=4) \n```\n\n### PROCEDURE:  READ FIRST BEFORE STARTING!!!\n\nThis midterm is due Wednesday, November 22 at midnight -   no exceptions.  Your\nassignment should be turned in to the GitHub repo you created based on the\nassignment, similar to the HW assignments.\n\nStrict rule: You are to do these problems ENTIRELY ON YOUR OWN - with no human\nhelp.  You may consult your textbook, class handouts, solutions to homeworks,\nand any notes which *you yourself personally* wrote.  You may NOT use any other\nbooks or other people's notes.  You may use a calculator.  You can take as long\nas you like to do this exam, but I don't want really want you to spend more than\nan hour or two (after all, you have a life!)  You may read through the problems\nand cogitate on them before starting the fun activity.\n\nThe only person from whom you may solicit help is ME.  You can do this by email\n(`jo.hardin@pomona.edu`), by telephone (9096078717) or in person (Millikan\n2351).  Be sure to ask if you don't understand something.\n\nPlease remember to explain your reasoning, when appropriate.  Don't write\nextraneous things in your answers - keep them concise and clear.   Your work\nshould be done in using Markdown in R Studio and compiled as a single\nreproducible document (including code that can be run 100%).  There should be a\nnarrative surrounding the code and explaining the results of the output.\n\nDo not include pages of superfluous output.\n\n*Internet:*  You may not use the internet for anything except the R software\nprogram (okay to use stack exchange to figure out programming errors; okay to\ngoogle to figure out plot / dplyr commands, etc.)); the HW solutions on Sakai;\nand the course website.  The main idea here: no googling how to **do** the\nproblem.\n\n\nHonor is an important part of life at Claremont. I agree that I will not discuss\nthis test with anyone until after we have both turned it in.\n\nNAME (signature):  Julian DeGroot-Lutzner\n\nTIME AND DATE STARTED:   Nov 21, 10:15 am\n\nTIME (AND DATE)  FINISHED: Nov 22, 2:30 pm\n\nEXTRA COMMENTS (optional):\n\n\n## Get the data\n\nNote: USE THE DATA ON GITHUB!\n\nYour task is to classify movies as to whether or not they won an Oscar (not all\ncategories); use the training data provided.  Separately, you will predict the\ntest data provided, and the strength of your prediction will be based on the\nsecret test labels that are hidden.\n\n## Creating the training and test data\n\n\n```{r} \nlibrary(tidyverse)\nlibrary(caret)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart.plot)\n```\n\n## Choice of explanatory variables\n\n\n```{r data}\n\nmovietrain <- read.csv(\"movietrain.csv\", sep=\",\") \nmovietest <- read.csv(\"movietest.csv\", sep=\",\")\n```\n\n\n* year: Year of release. \n* budget: Total budget (if known) in US dollars  \n* rating: Average IMDB user rating.\n* votes: Number of IMDB users who rated this movie. \n* r1-10: Multiplying by ten gives percentile (to nearest 10%) of IMDB users who rated this movie a 1. \n* action, animation, comedy, drama, documentary, romance, short: Binary variables representing if movie was classified as belonging to that genre. \n* audience_score: Audience score on Rotten Tomatoes \n* type: Type of movie (Documentary, Feature Film, TV Movie) \n* genre: Genre of movie (Action & Adventure, Comedy, Documentary, Drama, Horror, Mystery & Suspense, Other) \n* runtime: Runtime of movie (in minutes) \n* mpaa_rating: MPAA rating of the movie (G, PG, PG-13, R, Unrated) \n* studio: Studio that produced the movie \n* critics_score: Critics score on Rotten Tomatoes \n* critics_rating: Categorical variable for critics rating on Rotten Tomatoes (Certified Fresh, Fresh, Rotten) \n* top200_box: Whether or not the movie is in the Top 200 Box Office list on BoxOfficeMojo (no, yes) \n* audience_rating: Categorical variable for audience rating on Rotten Tomatoes (Spilled, Upright) \n* oscar: whether or not the movie won an Academy Award [not every category is considered here in creating the variable]\n\n\n1. (+4 points) After looking at the dataset, think about which ones should be\nremoved prior to running a classification.  Provide a few sentences describing\nthe variable choices you made. Note that the variables choices may change\ndepending on which classification model you use below.  You are not required to\nvisualize the variables, but if you use a graphic to decide how to use the\nvariable, you should include the graphic in your results.  [You should not plan\nto do everything we've done in class.  However, you should be able to explain\nwhy you did what you did.]\n\n```{r} \n\ntrain.by.oscar <- movietrain%>% group_by(oscar)\ntrain.by.oscar %>% \n  summarize(numMovies = n(),\n            numNA = sum(is.na(budget)), \n            avgBudget = mean(budget, na.rm = TRUE),\n            minBudget = min(budget, na.rm = TRUE),\n            maxBudget = max(budget, na.rm = TRUE))\n\nmovietrain <- movietrain %>% \nmutate(budgetUnder3Mil = as.numeric(ifelse(is.na(budget), \n                                           FALSE,\n                                           budget < 3000000)))\nmovietrain <- movietrain %>% select(-budget)\n``` \nMuch of the budget data is missing but I did not want to remove it completely because I hypothesized that there is a coorelation between budget and if a movie won an Oscar. Looking at the table above, there are 154 observations in the training set and 100 observations in the testing set. Of the 154 observations in the training set 50 won an Oscar and 104 did not. About half of each group, win or nowin, do not have the the budget reported so there is no interesting coorelation with missing data. Also both categories have films with high budgets so I did not see a coorelation there. The interesting statistic is that the smallest budget to win an Oscar was $3 million in the training set. I  created a new variable 'budgetUnder3Mil' for movies that have a known budget under 3 million.\n\n```{r}\nmovietrain <- movietrain %>% select(-studio)\n```\nStudio should be removed because it is a factor with 63 levels. Knowing 63 different types of Studios will lead to overfitting as many Studios are only attached to one movie. \n\n```{r}\nmovietrain <- movietrain %>% mutate(noDvd = as.numeric(is.na(dvd_rel_year)))\nmovietrain <- movietrain %>% select(-year, -thtr_rel_month, -thtr_rel_day,\n                                    -dvd_rel_year, -dvd_rel_month, -dvd_rel_day)\n```\nYear and other date information should be removed as they will lead to overfitting. Year has the opposite influence on the test data than we want. A movie is not chosen because of the year it was made; every year movies are chosen for an Oscar. A movie in the test set that is from the same year as a movie that won an Oscar in the training set will probably be at a lower chance of winning an Oscar because another movie already won one.\n\n```{r}\nmovietrain <- movietrain %>% select(-genre)\n```\nGenre has some repetitive variables that are already included in the binary variables action, animination, etc. These repetitive variables should be removed so that they do not overweight their influence. The genres that are not included as binary variables could be changed into binary variables but many of the genres do not align with the binary descriptions (Ie. a movie with Genre \"Drama\" has the binary variables \"Comedy\" and \"Romance\" as true). For simplicity sake, I will remove the whole Genre column. \n\n\n```{r}\noscar<-movietrain$oscar\nmovietrain <- movietrain %>% mutate(\n  top200_box = as.numeric(ifelse(top200_box == \"yes\", TRUE, FALSE)))\ndummy <- dummyVars(oscar~., movietrain)\nmovietrain <- predict(dummy, newdata = movietrain)\nmovietrain <- cbind(oscar,data.frame(movietrain))\n\n```\n\nSVM requires all of the data to be quantitative and the dummyVars function changed the dataset into one that is ready to use. More analysis can be done on feature selection and extraction including tuning these selections by running models with different variables. For the sake of the midterm, I have shown the thought process that would be involved in improving the model accuracy due to the variables. \n\n## Run the models\n\n2. (+9 points) The main task for this problem is to produce a model which will\nperform well on the secret hidden data.  As described above, you'll have to\ndecide on a type of classification model, on the tuning parameters, and on the\nvariables to put into the model. Describe your choices for which model you used,\nhow you found appropriate tuning parameters, etc.  You should try at least two\ndifferent models, each with a variety of tuning parameters.\n\nNote: some of the models take a long time to train.  Do not try to make a super\nfine grid for the tuning parameter search.\n\nPartition the Data into Training Set and Test Set\n```{r}\nset.seed(47)\ninTrain <- createDataPartition(movietrain$oscar, p = 0.7, list=FALSE)\ntraining <- movietrain[inTrain, ]\ntesting <- movietrain[-inTrain,]\n```\n\nRandom Forest\n\nHow many trees is enough?\n```{r}\nresults <- data.frame()\nfor(n in seq(from= 50, to = 650, by= 100)){\nset.seed(47)\nrf.model <- train(oscar~., data = training, method = \"rf\", \n                     trControl=trainControl(method=\"oob\"),\n                     ntree = n, tuneGrid=data.frame(mtry=6))\nprediction <- predict(rf.model, testing)\ncm <- confusionMatrix(prediction, testing$oscar)\nresults <- rbind(results , cm$overall)\n}\nresults\n```\nUse 300 trees.\n```{r}\nset.seed(47)\nrf.model <- train(oscar~., data = training, method = \"rf\", \n                     trControl=trainControl(method=\"oob\"),\n                     ntree = 300, tuneGrid=data.frame(mtry=1:39),\n                  importace = TRUE)\nrf.model$finalModel\n\n```\n```{r}\nrf.pred <- predict(rf.model, testing)\nconfusionMatrix(rf.pred, testing$oscar)\n```\n\n\n\n\nSVM\n```{r}\nset.seed(47)\nsvm.linear.model <- train(oscar~., data = training, method=\"svmLinear\", \n                 trControl = trainControl(method=\"cv\"),\n                 tuneGrid= expand.grid(C= c(0.01,0.1,1,10)),\n                 preProcess = c(\"center\", \"scale\"))\nsvm.linear.model\n```\n```{r}\nsvm.linear.pred <- predict(svm.linear.model, testing)\nconfusionMatrix(svm.linear.pred, testing$oscar)\n\n```\n```{r}\nset.seed(47)\nsvm.rad.model <- train(oscar ~ ., data= training, method=\"svmRadial\", \n                 trControl = trainControl(method=\"cv\"),\n                 tuneGrid= expand.grid(C=c(0.01,0.1,1,10,100),\n                                       sigma=c(.5,1,2,3,4)),\n                 preProcess = c(\"center\", \"scale\"))\n\nsvm.rad.pred <- predict(svm.rad.model, testing)\nconfusionMatrix(svm.rad.pred,testing$oscar)\n```\nThe model I will use to predict the test data is the SVM with the linear kernel. The radial kernel predicted all of the testing data as nowin. The SVM linear kernel model was better at predicting movies that won Oscars than the RandomForest model. In other words, the specificity of the SVM linear model was better than the specificity of the Random Forest model.\n\n## Predict\n\n3. (+3 points)  What do you expect your test error rate to be?  Explain.\n\nI expect the test error rate to be about 69%. I segmented the training data into a training set and a testing set. I used the training set to tune the parameters of each model and then I used the testing set to test the accuracy of the model. The SVM linear model was 69.6% accurate at predicting data it had not seen before. \n\n\n4. (+4 points) After you have set the model, predict the test data which is\nprovided in GitHub.  Create the vector of predictions in the *same* order as the\noriginal explanatory variables in the test dataset.  The vector you create\nshould be at the end of your markdown file.  The markdown file will be run, and\nyour predictions will be tested against the truth.\n\n```{r}\n# wrangle test data so it is the same as the input of the model \nmovietest <- movietest %>% mutate(budgetUnder3Mil = \n                                    as.numeric(ifelse(is.na(budget),\n                                                      FALSE,\n                                                      budget < 3000000)),\n                                  noDvd = as.numeric(is.na(dvd_rel_year)),\n                                  top200_box = \n                                    as.numeric(ifelse(top200_box == \"yes\",\n                                                      TRUE, \n                                                      FALSE)))\nmovietest <- movietest %>% select( -thtr_rel_month, -thtr_rel_day, \n                                   -dvd_rel_year, -dvd_rel_month, -dvd_rel_day,\n                                   -budget, -genre, -studio )\ndummy <- dummyVars(year~., movietest)\nmovietest <- data.frame(predict(dummy, newdata = movietest))\nmovietest <- cbind(movietest, data.frame(mpaa_rating.NC.17 = numeric(100)))\n\n```\n\n```{r}\n# predict \nfinal.prediction <- predict(svm.linear.model, movietest)\nfinal.prediction\n```\n",
    "created" : 1512724779577.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3211381907",
    "id" : "14194F02",
    "lastKnownWriteTime" : 1511389992,
    "last_content_update" : 1511389992,
    "path" : "~/Documents/math154/homework/ma154-takehome2-jdegrootlutzner/ma154-takehome2-degrootlutzner-julian.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}